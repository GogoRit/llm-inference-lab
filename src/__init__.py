"""
LLM Inference Lab - Main Package

This package provides a comprehensive toolkit for optimizing and benchmarking
Large Language Model inference performance with advanced techniques including
speculative decoding, custom CUDA kernels, and intelligent batching.

Main modules:
- server: API & serving infrastructure
- scheduler: Request batching & routing
- specdec: Speculative decoding implementation
- kernels: Custom CUDA kernels
- benchmarks: Performance testing tools
"""

__version__ = "0.1.0"
__author__ = "LLM Inference Lab Team"
