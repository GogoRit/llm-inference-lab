"""
Server Module - API & Serving Infrastructure

This module contains the core serving infrastructure for the LLM inference lab,
including FastAPI-based REST API, WebSocket support, and Streamlit web interface.

Components:
- FastAPI application with REST endpoints
- WebSocket support for streaming responses
- Streamlit dashboard for monitoring and control
- Request/response models and validation
- Authentication and rate limiting
- Health checks and metrics endpoints
"""

# Import modules when they exist
# from .main import app, create_app
# from .models import InferenceRequest, InferenceResponse, HealthResponse

# __all__ = [
#     "app",
#     "create_app",
#     "InferenceRequest",
#     "InferenceResponse",
#     "HealthResponse",
# ]
